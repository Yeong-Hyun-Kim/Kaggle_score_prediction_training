{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4e7f59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라이브러리 임포트\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import koreanize_matplotlib\n",
    "koreanize_matplotlib.koreanize()\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "793bde7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (630000, 13), Test: (270000, 12)\n"
     ]
    }
   ],
   "source": [
    "# 데이터 로드\n",
    "train = pd.read_csv(r'E:\\2026_1\\캐글 공모전\\playground-series-s6e1\\train.csv')\n",
    "test = pd.read_csv(r'E:\\2026_1\\캐글 공모전\\playground-series-s6e1\\test.csv')\n",
    "submission = pd.read_csv(r'E:\\2026_1\\캐글 공모전\\playground-series-s6e1\\sample_submission.csv')\n",
    "\n",
    "print(f\"Train: {train.shape}, Test: {test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9466e5a7",
   "metadata": {},
   "source": [
    "## 전략: six_try 기반 추가 개선\n",
    "\n",
    "six_try 최고 성능: CV RMSE 8.73352 (LGB, num_leaves=60, min_child_samples=35, Top 15 피처)\n",
    "\n",
    "이번에 시도할 것들:\n",
    "1. 전체 피처 사용 + 최적 파라미터\n",
    "2. XGBoost 추가\n",
    "3. LightGBM + XGBoost 스태킹/블렌딩\n",
    "4. 잔차 분석 후 타깃별 보정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58ea0f96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "파생변수 생성 완료: 34개 컬럼\n"
     ]
    }
   ],
   "source": [
    "# 파생변수 생성 (six_try와 동일)\n",
    "def create_features(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    # 범주형 인코딩\n",
    "    sleep_map = {'poor': 1, 'average': 2, 'good': 3}\n",
    "    facility_map = {'low': 1, 'medium': 2, 'high': 3}\n",
    "    difficulty_map = {'easy': 1, 'moderate': 2, 'hard': 3}\n",
    "    \n",
    "    df['sleep_quality_num'] = df['sleep_quality'].map(sleep_map)\n",
    "    df['facility_num'] = df['facility_rating'].map(facility_map)\n",
    "    df['difficulty_num'] = df['exam_difficulty'].map(difficulty_map)\n",
    "    \n",
    "    # 조건부 상호작용\n",
    "    df['study_quality_adj'] = df['study_hours'] * (df['sleep_quality_num'] / 3)\n",
    "    df['study_facility_adj'] = df['study_hours'] * (df['facility_num'] / 3)\n",
    "    df['attendance_sleep_synergy'] = (df['class_attendance'] / 100) * df['sleep_quality_num']\n",
    "    \n",
    "    # 효율성 지표\n",
    "    df['waking_study_ratio'] = df['study_hours'] / (24 - df['sleep_hours'])\n",
    "    df['study_sleep_ratio'] = df['study_hours'] / (df['sleep_hours'] + 0.1)\n",
    "    df['total_investment'] = df['study_hours'] + df['sleep_hours']\n",
    "    df['prep_vs_difficulty'] = df['study_hours'] / (df['difficulty_num'] + 0.5)\n",
    "    \n",
    "    # 비선형 변환\n",
    "    df['study_sqrt'] = np.sqrt(df['study_hours'])\n",
    "    df['study_log'] = np.log1p(df['study_hours'])\n",
    "    df['study_sq'] = df['study_hours'] ** 2\n",
    "    df['sleep_sq'] = df['sleep_hours'] ** 2\n",
    "    \n",
    "    # 최적 구간 거리\n",
    "    df['study_optimal_dist'] = abs(df['study_hours'] - 5)\n",
    "    df['sleep_deficit'] = np.maximum(0, 7 - df['sleep_hours'])\n",
    "    df['sleep_excess'] = np.maximum(0, df['sleep_hours'] - 8)\n",
    "    \n",
    "    # 학생 프로파일\n",
    "    df['hardworking_type'] = ((df['study_hours'] > 5) & (df['class_attendance'] > 85)).astype(int)\n",
    "    df['cramming_type'] = ((df['study_hours'] > 6) & (df['sleep_hours'] < 6)).astype(int)\n",
    "    \n",
    "    # 출석 임계점\n",
    "    df['attendance_low'] = (df['class_attendance'] < 70).astype(int)\n",
    "    df['attendance_high'] = (df['class_attendance'] >= 90).astype(int)\n",
    "    \n",
    "    return df\n",
    "\n",
    "train = create_features(train)\n",
    "test = create_features(test)\n",
    "print(f\"파생변수 생성 완료: {train.shape[1]}개 컬럼\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52a23ea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "라벨 인코딩 완료\n"
     ]
    }
   ],
   "source": [
    "# 범주형 라벨 인코딩\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "cat_cols = ['gender', 'course', 'sleep_quality', 'study_method', \n",
    "            'facility_rating', 'exam_difficulty', 'internet_access']\n",
    "\n",
    "label_encoders = {}\n",
    "for col in cat_cols:\n",
    "    le = LabelEncoder()\n",
    "    train[col] = le.fit_transform(train[col])\n",
    "    test[col] = le.transform(test[col])\n",
    "    label_encoders[col] = le\n",
    "\n",
    "print(\"라벨 인코딩 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5ea072a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 피처: 32개\n",
      "선택 피처: 15개\n"
     ]
    }
   ],
   "source": [
    "# 피처/타겟 분리\n",
    "drop_cols = ['id', 'exam_score']\n",
    "feature_cols = [col for col in train.columns if col not in drop_cols]\n",
    "\n",
    "X = train[feature_cols]\n",
    "y = train['exam_score']\n",
    "X_test = test[feature_cols]\n",
    "\n",
    "# six_try에서 선정된 Top 15 피처 (직접 지정)\n",
    "selected_features_15 = [\n",
    "    'waking_study_ratio', 'study_facility_adj', 'class_attendance',\n",
    "    'attendance_sleep_synergy', 'study_hours', 'study_quality_adj',\n",
    "    'study_method', 'study_log', 'total_investment', 'attendance_low',\n",
    "    'facility_num', 'facility_rating', 'study_sqrt', 'hardworking_type',\n",
    "    'study_optimal_dist'\n",
    "]\n",
    "\n",
    "print(f\"전체 피처: {len(feature_cols)}개\")\n",
    "print(f\"선택 피처: {len(selected_features_15)}개\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a2e38c",
   "metadata": {},
   "source": [
    "## 1. LightGBM Baseline (six_try 최적 파라미터)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6fe9fd4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's rmse: 8.82564\n",
      "[200]\tvalid_0's rmse: 8.77343\n",
      "[300]\tvalid_0's rmse: 8.7541\n",
      "[400]\tvalid_0's rmse: 8.7448\n",
      "[500]\tvalid_0's rmse: 8.73769\n",
      "[600]\tvalid_0's rmse: 8.7319\n",
      "[700]\tvalid_0's rmse: 8.72693\n",
      "[800]\tvalid_0's rmse: 8.7241\n",
      "[900]\tvalid_0's rmse: 8.72189\n",
      "[1000]\tvalid_0's rmse: 8.72023\n",
      "[1100]\tvalid_0's rmse: 8.71967\n",
      "[1200]\tvalid_0's rmse: 8.71917\n",
      "[1300]\tvalid_0's rmse: 8.71815\n",
      "[1400]\tvalid_0's rmse: 8.71808\n",
      "Early stopping, best iteration is:\n",
      "[1388]\tvalid_0's rmse: 8.71784\n",
      "\n",
      "LightGBM Valid RMSE: 8.71784\n"
     ]
    }
   ],
   "source": [
    "# LightGBM 학습 (six_try 최적 파라미터)\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Train/Valid Split\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X[selected_features_15], y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# six_try 최적 파라미터\n",
    "lgb_params = {\n",
    "    'objective': 'regression',\n",
    "    'metric': 'rmse',\n",
    "    'learning_rate': 0.05,\n",
    "    'num_leaves': 60,\n",
    "    'max_depth': -1,\n",
    "    'min_child_samples': 35,\n",
    "    'feature_fraction': 0.8,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'verbose': -1,\n",
    "    'random_state': 42\n",
    "}\n",
    "\n",
    "train_data = lgb.Dataset(X_train, label=y_train)\n",
    "valid_data = lgb.Dataset(X_valid, label=y_valid, reference=train_data)\n",
    "\n",
    "lgb_model = lgb.train(\n",
    "    lgb_params,\n",
    "    train_data,\n",
    "    num_boost_round=2000,\n",
    "    valid_sets=[valid_data],\n",
    "    callbacks=[lgb.early_stopping(100), lgb.log_evaluation(100)]\n",
    ")\n",
    "\n",
    "lgb_pred_valid = lgb_model.predict(X_valid)\n",
    "lgb_pred_test = lgb_model.predict(X_test[selected_features_15])\n",
    "\n",
    "lgb_rmse = np.sqrt(mean_squared_error(y_valid, lgb_pred_valid))\n",
    "print(f\"\\nLightGBM Valid RMSE: {lgb_rmse:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df647db9",
   "metadata": {},
   "source": [
    "## 2. XGBoost 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45e5e9c2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xgboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# XGBoost 학습\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mxgboost\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mxgb\u001b[39;00m\n\u001b[0;32m      4\u001b[0m xgb_params \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobjective\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreg:squarederror\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124meval_metric\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrmse\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_jobs\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     14\u001b[0m }\n\u001b[0;32m     16\u001b[0m dtrain \u001b[38;5;241m=\u001b[39m xgb\u001b[38;5;241m.\u001b[39mDMatrix(X_train, label\u001b[38;5;241m=\u001b[39my_train)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'xgboost'"
     ]
    }
   ],
   "source": [
    "# XGBoost 학습\n",
    "import xgboost as xgb\n",
    "\n",
    "xgb_params = {\n",
    "    'objective': 'reg:squarederror',\n",
    "    'eval_metric': 'rmse',\n",
    "    'learning_rate': 0.05,\n",
    "    'max_depth': 8,\n",
    "    'min_child_weight': 35,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'random_state': 42,\n",
    "    'n_jobs': -1\n",
    "}\n",
    "\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dvalid = xgb.DMatrix(X_valid, label=y_valid)\n",
    "dtest = xgb.DMatrix(X_test[selected_features_15])\n",
    "\n",
    "xgb_model = xgb.train(\n",
    "    xgb_params,\n",
    "    dtrain,\n",
    "    num_boost_round=2000,\n",
    "    evals=[(dvalid, 'valid')],\n",
    "    early_stopping_rounds=100,\n",
    "    verbose_eval=100\n",
    ")\n",
    "\n",
    "xgb_pred_valid = xgb_model.predict(dvalid)\n",
    "xgb_pred_test = xgb_model.predict(dtest)\n",
    "\n",
    "xgb_rmse = np.sqrt(mean_squared_error(y_valid, xgb_pred_valid))\n",
    "print(f\"\\nXGBoost Valid RMSE: {xgb_rmse:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634377d5",
   "metadata": {},
   "source": [
    "## 3. LightGBM + XGBoost 블렌딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7cc6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최적 블렌딩 비율 탐색\n",
    "print(\"블렌딩 비율 탐색 (LightGBM : XGBoost)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "best_ratio = 0\n",
    "best_blend_rmse = float('inf')\n",
    "\n",
    "for lgb_ratio in np.arange(0, 1.05, 0.1):\n",
    "    xgb_ratio = 1 - lgb_ratio\n",
    "    blend_pred = lgb_ratio * lgb_pred_valid + xgb_ratio * xgb_pred_valid\n",
    "    blend_rmse = np.sqrt(mean_squared_error(y_valid, blend_pred))\n",
    "    \n",
    "    if blend_rmse < best_blend_rmse:\n",
    "        best_blend_rmse = blend_rmse\n",
    "        best_ratio = lgb_ratio\n",
    "    \n",
    "    print(f\"  LGB {lgb_ratio:.1f} : XGB {xgb_ratio:.1f} -> RMSE: {blend_rmse:.5f}\")\n",
    "\n",
    "print(f\"\\n최적 비율: LGB {best_ratio:.1f} : XGB {1-best_ratio:.1f}\")\n",
    "print(f\"최적 RMSE: {best_blend_rmse:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c1f032",
   "metadata": {},
   "source": [
    "## 4. 전체 피처 사용 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971fc966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 피처로 LightGBM 학습\n",
    "X_train_full, X_valid_full, y_train_full, y_valid_full = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "train_data_full = lgb.Dataset(X_train_full, label=y_train_full)\n",
    "valid_data_full = lgb.Dataset(X_valid_full, label=y_valid_full, reference=train_data_full)\n",
    "\n",
    "lgb_model_full = lgb.train(\n",
    "    lgb_params,\n",
    "    train_data_full,\n",
    "    num_boost_round=2000,\n",
    "    valid_sets=[valid_data_full],\n",
    "    callbacks=[lgb.early_stopping(100), lgb.log_evaluation(100)]\n",
    ")\n",
    "\n",
    "lgb_pred_valid_full = lgb_model_full.predict(X_valid_full)\n",
    "lgb_pred_test_full = lgb_model_full.predict(X_test)\n",
    "\n",
    "lgb_rmse_full = np.sqrt(mean_squared_error(y_valid_full, lgb_pred_valid_full))\n",
    "print(f\"\\nLightGBM (전체 피처) Valid RMSE: {lgb_rmse_full:.5f}\")\n",
    "print(f\"LightGBM (Top 15) Valid RMSE: {lgb_rmse:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a944aaa",
   "metadata": {},
   "source": [
    "## 5. 결과 비교 및 제출 파일 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f8ac40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과 비교\n",
    "print(\"=\"*60)\n",
    "print(\"모델 성능 비교\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "results = [\n",
    "    {\"모델\": \"LightGBM (Top 15)\", \"Valid RMSE\": lgb_rmse},\n",
    "    {\"모델\": \"XGBoost (Top 15)\", \"Valid RMSE\": xgb_rmse},\n",
    "    {\"모델\": f\"블렌딩 (LGB {best_ratio:.1f}:XGB {1-best_ratio:.1f})\", \"Valid RMSE\": best_blend_rmse},\n",
    "    {\"모델\": \"LightGBM (전체 피처)\", \"Valid RMSE\": lgb_rmse_full},\n",
    "]\n",
    "\n",
    "results_df = pd.DataFrame(results).sort_values(\"Valid RMSE\")\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "# 최적 모델 선택\n",
    "best_model_name = results_df.iloc[0]['모델']\n",
    "best_model_rmse = results_df.iloc[0]['Valid RMSE']\n",
    "print(f\"\\n최적 모델: {best_model_name} (RMSE: {best_model_rmse:.5f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47eb7a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 제출 파일 생성 (최적 블렌딩)\n",
    "final_pred = best_ratio * lgb_pred_test + (1 - best_ratio) * xgb_pred_test\n",
    "\n",
    "submission['exam_score'] = final_pred\n",
    "out_path = r'E:\\2026_1\\캐글 공모전\\영현\\sbmission_result\\submission_seven_lgb_xgb_blend.csv'\n",
    "submission.to_csv(out_path, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"제출 파일 저장: {out_path}\")\n",
    "print(f\"\\n예측값 통계:\")\n",
    "print(submission['exam_score'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de710b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시각화: 모델별 RMSE 비교\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# 1. RMSE 바 차트\n",
    "ax1 = axes[0]\n",
    "model_names = results_df['모델'].tolist()\n",
    "rmse_values = results_df['Valid RMSE'].tolist()\n",
    "colors = ['#2ecc71', '#3498db', '#e74c3c', '#9b59b6'][:len(model_names)]\n",
    "\n",
    "bars = ax1.barh(model_names, rmse_values, color=colors)\n",
    "ax1.set_xlabel('Valid RMSE')\n",
    "ax1.set_title('모델별 Valid RMSE 비교')\n",
    "ax1.set_xlim(min(rmse_values) * 0.995, max(rmse_values) * 1.005)\n",
    "\n",
    "for bar, val in zip(bars, rmse_values):\n",
    "    ax1.text(val + 0.001, bar.get_y() + bar.get_height()/2, \n",
    "             f'{val:.5f}', va='center', fontsize=10)\n",
    "\n",
    "# 2. 예측값 분포\n",
    "ax2 = axes[1]\n",
    "ax2.hist(lgb_pred_test, bins=50, alpha=0.5, label='LightGBM', color='#3498db')\n",
    "ax2.hist(xgb_pred_test, bins=50, alpha=0.5, label='XGBoost', color='#e74c3c')\n",
    "ax2.hist(final_pred, bins=50, alpha=0.5, label='Blend', color='#2ecc71')\n",
    "ax2.set_xlabel('Predicted exam_score')\n",
    "ax2.set_ylabel('Count')\n",
    "ax2.set_title('Test 예측값 분포')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "please",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
